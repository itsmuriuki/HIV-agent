{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb0ef65-3424-4fa9-bb2f-b8eb872b6a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^3.0.1\u001b[39;22m for \u001b[36mpypdf2\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(1.3s)\u001b[39;22m\n",
      "\n",
      "\u001b[39;1mPackage operations\u001b[39;22m: \u001b[34m1\u001b[39m install, \u001b[34m0\u001b[39m updates, \u001b[34m0\u001b[39m removals\n",
      "\n",
      "  \u001b[34;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.0.1\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.0.1\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.0.1\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m61%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.0.1\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m91%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.0.1\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.0.1\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m-\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpypdf2\u001b[39m\u001b[39m (\u001b[39m\u001b[32m3.0.1\u001b[39m\u001b[39m)\u001b[39m\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b237af5f-eaa1-40a9-a52f-d66cfc9b5a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 608\n",
      "\n",
      "First chunk preview:\n",
      "Start position: 0\n",
      "Content preview:  \n",
      " \n",
      "\n",
      "\n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Kenya HIV Prevention and Treatment Guidelines, 2022  \n",
      " \n",
      "2022 Edition  \n",
      " \n",
      " \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Â© National AIDS & STI Control Program 2022  \n",
      " \n",
      "Th...\n",
      "Metadata: Kenya-ARV-Guidelines-2022-Final-1.pdf\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    \"\"\"\n",
    "    Split a sequence into overlapping chunks using a sliding window.\n",
    "    \n",
    "    Args:\n",
    "        seq: The sequence to chunk (typically a string)\n",
    "        size: Size of each chunk\n",
    "        step: Step size between chunks (overlap = size - step)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'start' index and 'chunk' content\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Read text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file (string or Path object)\n",
    "    \n",
    "    Returns:\n",
    "        String containing all text from the PDF\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_path)\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "    \n",
    "    if not pdf_path.suffix.lower() == '.pdf':\n",
    "        raise ValueError(f\"File is not a PDF: {pdf_path}\")\n",
    "    \n",
    "    text_content = []\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        for page_num, page in enumerate(pdf_reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            text_content.append(page_text)\n",
    "    \n",
    "    return '\\n'.join(text_content)\n",
    "\n",
    "\n",
    "def process_pdf_with_chunks(pdf_path, chunk_size=2000, step_size=1000, metadata=None):\n",
    "    \"\"\"\n",
    "    Read a PDF and create sliding window chunks with metadata.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        chunk_size: Size of each chunk (default: 2000 characters)\n",
    "        step_size: Step between chunks (default: 1000 characters)\n",
    "        metadata: Optional dict of metadata to add to each chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of chunk dictionaries with content and metadata\n",
    "    \"\"\"\n",
    "    # Read PDF content\n",
    "    content = read_pdf(pdf_path)\n",
    "    \n",
    "    # Create chunks using sliding window\n",
    "    chunks = sliding_window(content, chunk_size, step_size)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    if metadata is None:\n",
    "        metadata = {}\n",
    "    \n",
    "    # Add source file to metadata\n",
    "    metadata['source'] = str(Path(pdf_path).name)\n",
    "    metadata['full_path'] = str(Path(pdf_path).absolute())\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk.update(metadata)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Example usage for multiple PDF files\n",
    "def process_multiple_pdfs(pdf_paths, chunk_size=2000, step_size=1000):\n",
    "    \"\"\"\n",
    "    Process multiple PDF files and combine their chunks.\n",
    "    \n",
    "    Args:\n",
    "        pdf_paths: List of paths to PDF files\n",
    "        chunk_size: Size of each chunk\n",
    "        step_size: Step between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of all chunks from all PDFs\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        try:\n",
    "            # You can add custom metadata per document here\n",
    "            metadata = {\n",
    "                'document_id': Path(pdf_path).stem,  # filename without extension\n",
    "            }\n",
    "            \n",
    "            chunks = process_pdf_with_chunks(pdf_path, chunk_size, step_size, metadata)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            print(f\"Processed {pdf_path}: {len(chunks)} chunks created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 1: Process a single PDF\n",
    "    pdf_file = \"Kenya-ARV-Guidelines-2022-Final-1.pdf\"\n",
    "    \n",
    "    try:\n",
    "        chunks = process_pdf_with_chunks(\n",
    "            pdf_file,\n",
    "            chunk_size=2000,\n",
    "            step_size=1000,\n",
    "            metadata={'category': 'documentation', 'version': '1.0'}\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTotal chunks: {len(chunks)}\")\n",
    "        print(f\"\\nFirst chunk preview:\")\n",
    "        print(f\"Start position: {chunks[0]['start']}\")\n",
    "        print(f\"Content preview: {chunks[0]['chunk'][:200]}...\")\n",
    "        print(f\"Metadata: {chunks[0].get('source')}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Please provide a valid PDF file path\")\n",
    "    \n",
    "    # Example 2: Process multiple PDFs (matching your original pattern)\n",
    "    \"\"\"\n",
    "    evidently_docs = [\n",
    "        {'path': 'doc1.pdf', 'author': 'John', 'type': 'report'},\n",
    "        {'path': 'doc2.pdf', 'author': 'Jane', 'type': 'analysis'},\n",
    "    ]\n",
    "    \n",
    "    evidently_chunks = []\n",
    "    \n",
    "    for doc in evidently_docs:\n",
    "        doc_copy = doc.copy()\n",
    "        pdf_path = doc_copy.pop('path')  # Get the path and remove from metadata\n",
    "        \n",
    "        # Read PDF and create chunks\n",
    "        content = read_pdf(pdf_path)\n",
    "        chunks = sliding_window(content, 2000, 1000)\n",
    "        \n",
    "        # Add metadata to each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        \n",
    "        evidently_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"\\nProcessed {len(evidently_docs)} documents into {len(evidently_chunks)} chunks\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b747410a-ef02-4db1-9aed-549fc08313e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evidently_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m text = \u001b[43mevidently_docs\u001b[49m[\u001b[32m45\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m paragraphs = re.split(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m, text.strip())\n",
      "\u001b[31mNameError\u001b[39m: name 'evidently_docs' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41de984d-fedd-4542-afbb-46c9090bc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237d6b21-24a5-4560-a451-887a9c122f71",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m sections = split_markdown_by_level(\u001b[43mtext\u001b[49m, level=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5e9f6-cde1-4031-b729-5cd4a9e45b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kenya-hiv-cdss)",
   "language": "python",
   "name": "kenya-hiv-cdss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
